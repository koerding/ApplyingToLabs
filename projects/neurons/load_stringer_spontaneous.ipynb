{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koerding/ApplyingToLabs/blob/main/projects/neurons/load_stringer_spontaneous.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "QSCa1nbKAXEb"
      },
      "source": [
        "## Universal data properties relevant to causal inference\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Targets=['Hexaco','360PDA','Stringer','RNASeq']\n",
        "Target=Targets[3]\n",
        "print(Target)"
      ],
      "metadata": {
        "id": "wtmaE2ynGVUm",
        "outputId": "4c577f5e-67fe-4f59-b25f-34634faffc48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNASeq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {},
        "id": "KIFFamASAXEf"
      },
      "outputs": [],
      "source": [
        "# @title Load dependencies\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "from scipy.stats import zscore\n",
        "from sklearn.decomposition import PCA\n",
        "import os\n",
        "import zipfile\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_covariance(data):\n",
        "    \"\"\"\n",
        "    Compute the covariance matrix. If the data is sparse\n",
        "    (mostly zeros), use a Poisson-adjusted correlation matrix instead.\n",
        "\n",
        "    Parameters:\n",
        "    - data (numpy.ndarray): Input data matrix where rows are samples and columns are features.\n",
        "\n",
        "    Returns:\n",
        "    - cov_matrix (numpy.ndarray): Covariance or Poisson-adjusted correlation matrix.\n",
        "    \"\"\"\n",
        "    def is_sparse(matrix, threshold=0.5):\n",
        "        \"\"\"Check if the matrix is sparse (fraction of zeros exceeds threshold).\"\"\"\n",
        "        zero_fraction = np.sum(matrix == 0) / matrix.size\n",
        "        return zero_fraction > threshold\n",
        "\n",
        "    def poisson_adjusted_correlation(data):\n",
        "        \"\"\"Compute Poisson-adjusted correlation matrix.\"\"\"\n",
        "        means = np.mean(data, axis=0)\n",
        "        stds = np.sqrt(means)  # Poisson standard deviation\n",
        "        normalized_data = (data - means) / stds\n",
        "        normalized_cov = np.cov(normalized_data, rowvar=False)\n",
        "        adjusted_corr = normalized_cov / np.outer(stds, stds)\n",
        "        return adjusted_corr\n",
        "\n",
        "    # Determine if the data is sparse\n",
        "    sparse = is_sparse(data)\n",
        "\n",
        "    if sparse:\n",
        "        print(\"Data is sparse. Using Poisson-adjusted correlation matrix.\")\n",
        "        cov_matrix = poisson_adjusted_correlation(data)\n",
        "    else:\n",
        "        print(\"Data is dense. Using standard covariance calculation.\")\n",
        "        cov_matrix = np.cov(data, rowvar=False) / np.outer(\n",
        "            np.std(data, axis=0, ddof=1), np.std(data, axis=0, ddof=1)\n",
        "        )\n",
        "\n",
        "    return cov_matrix\n"
      ],
      "metadata": {
        "id": "EJf-5N-Mwq85"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "POuIuVBrAXEf"
      },
      "outputs": [],
      "source": [
        "# @title Figure settings\n",
        "from matplotlib import rcParams\n",
        "\n",
        "rcParams['figure.figsize'] = [20, 4]\n",
        "rcParams['font.size'] =15\n",
        "rcParams['axes.spines.top'] = False\n",
        "rcParams['axes.spines.right'] = False\n",
        "rcParams['figure.autolayout'] = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_rnaseq_data():\n",
        "    \"\"\"\n",
        "    Load RNASeq data, clean it, visualize it, and return the gene-by-cluster matrix.\n",
        "\n",
        "    Returns:\n",
        "    - data_matrix (numpy.ndarray): Cleaned gene-by-cluster expression matrix.\n",
        "    \"\"\"\n",
        "    # Step 1: Download the file\n",
        "    url = \"https://www.dropbox.com/scl/fi/6pcw5lztnp9l0pcvjfiyt/medians.csv?rlkey=5997l65mvqczuqhbl87uqutfk&dl=1\"\n",
        "    local_file = \"medians.csv\"\n",
        "\n",
        "    if not os.path.isfile(local_file):\n",
        "        print(\"Downloading the file...\")\n",
        "        response = requests.get(url)\n",
        "        with open(local_file, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(\"Download complete.\")\n",
        "\n",
        "    # Step 2: Load the dataset\n",
        "    medians_df = pd.read_csv(local_file)\n",
        "    # Set the first column as the index (gene names)\n",
        "    medians_df.set_index(medians_df.columns[0], inplace=True)\n",
        "    # Remove trivial rows (all zero or mostly zero)\n",
        "    medians_df = medians_df.loc[medians_df.sum(axis=1) > 0]\n",
        "    # Remove trivial columns (all zero or mostly zero)\n",
        "    medians_df = medians_df.loc[:, medians_df.sum(axis=0) > 0]\n",
        "\n",
        "    # Step 3: Extract the gene-by-cluster matrix\n",
        "    gene_names = medians_df.index  # Gene names (row identifiers)\n",
        "    expression_matrix = medians_df.to_numpy()  # Expression values\n",
        "\n",
        "    # Return the cleaned matrix\n",
        "    data_matrix = expression_matrix\n",
        "    return data_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "DwhZBbwINo1_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IpdXBYaHpl43"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_hexaco_data():\n",
        "    \"\"\"\n",
        "    Load the HEXACO personality dataset, clean it, visualize it, and return the numeric data matrix.\n",
        "\n",
        "    Returns:\n",
        "    - data_matrix (numpy.ndarray): Cleaned HEXACO data matrix containing numeric columns only.\n",
        "    \"\"\"\n",
        "    # Step 1: Download the ZIP file\n",
        "    hexaco_url = \"https://www.dropbox.com/scl/fi/gpis8v7ojcwegqqco9ede/HEXACO.zip?rlkey=tnupayuu8bpwfgw8i50xtbub8&dl=1\"\n",
        "    hexaco_zip = \"HEXACO.zip\"\n",
        "\n",
        "    if not os.path.isfile(hexaco_zip):\n",
        "        print(\"Downloading HEXACO dataset...\")\n",
        "        response = requests.get(hexaco_url)\n",
        "        with open(hexaco_zip, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(\"Download complete.\")\n",
        "\n",
        "    # Step 2: Extract the ZIP file\n",
        "    extracted_folder = \"HEXACO_extracted\"\n",
        "    if not os.path.isdir(extracted_folder):\n",
        "        with zipfile.ZipFile(hexaco_zip, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(extracted_folder)\n",
        "        print(f\"Extracted to {extracted_folder}.\")\n",
        "\n",
        "    # Step 3: Load the data\n",
        "    hexaco_data_path = os.path.join(extracted_folder, \"HEXACO\", \"data.csv\")\n",
        "    hexaco_df = pd.read_csv(hexaco_data_path, sep=\"\\t\")\n",
        "\n",
        "    # Step 4: Clean the data (numeric columns only)\n",
        "    hexaco_numeric_data = hexaco_df.select_dtypes(include=[np.number]).to_numpy()\n",
        "\n",
        "    # Return the cleaned numeric data matrix\n",
        "    data_matrix = hexaco_numeric_data\n",
        "    return data_matrix\n"
      ],
      "metadata": {
        "id": "4TDXdaTkAxSj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pda360_data():\n",
        "    \"\"\"\n",
        "    Load the 360PDA dataset, clean it, visualize it, and return the cleaned data matrix.\n",
        "\n",
        "    Returns:\n",
        "    - data_matrix (numpy.ndarray): Cleaned 360PDA data matrix with no missing rows.\n",
        "    \"\"\"\n",
        "    # Step 1: Download the 360PDA dataset\n",
        "    pda_url = \"https://www.dropbox.com/scl/fi/53s5j5pbwkwxgjm3ywi9s/360PDA.tab?rlkey=3lrpa8zumxxbn5y5hkrfubi1i&dl=1\"\n",
        "    pda_file = \"360PDA.tab\"\n",
        "\n",
        "    if not os.path.isfile(pda_file):\n",
        "        print(\"Downloading 360PDA dataset...\")\n",
        "        response = requests.get(pda_url)\n",
        "        with open(pda_file, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(\"Download complete.\")\n",
        "\n",
        "    # Step 2: Load the dataset\n",
        "    pda_df = pd.read_csv(pda_file, sep=\"\\t\")\n",
        "\n",
        "    # Step 3: Clean the dataset\n",
        "    # Exclude non-behavioral columns like \"ID\" if present\n",
        "    if \"ID\" in pda_df.columns:\n",
        "        pda_behavioral_data = pda_df.drop(columns=[\"ID\"]).select_dtypes(include=[np.number]).to_numpy()\n",
        "    else:\n",
        "        pda_behavioral_data = pda_df.select_dtypes(include=[np.number]).to_numpy()\n",
        "\n",
        "    # Remove rows with at least one missing value\n",
        "    pda_cleaned_no_missing = pda_behavioral_data[~np.any(np.isnan(pda_behavioral_data), axis=1)]\n",
        "\n",
        "    # Return the cleaned matrix\n",
        "    data_matrix = pda_cleaned_no_missing\n",
        "    return data_matrix\n"
      ],
      "metadata": {
        "id": "o698UE3ODusG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {},
        "id": "KlIJ_2tQAXEf"
      },
      "outputs": [],
      "source": [
        "def load_stringer_data():\n",
        "    \"\"\"\n",
        "    Load the Stringer spontaneous activity neuroscience dataset, visualize behavioral properties, and return the neural response matrix.\n",
        "\n",
        "    Returns:\n",
        "    - data_matrix (numpy.ndarray): Neural response matrix (`sresp`).\n",
        "    \"\"\"\n",
        "    # Step 1: Data retrieval\n",
        "    fname = \"stringer_spontaneous.npy\"\n",
        "    url = \"https://osf.io/dpqaj/download\"\n",
        "\n",
        "    if not os.path.isfile(fname):\n",
        "        try:\n",
        "            print(\"Downloading Stringer dataset...\")\n",
        "            r = requests.get(url)\n",
        "        except requests.ConnectionError:\n",
        "            raise Exception(\"Failed to download data!\")\n",
        "        else:\n",
        "            if r.status_code != requests.codes.ok:\n",
        "                raise Exception(\"Failed to download data!\")\n",
        "            else:\n",
        "                with open(fname, \"wb\") as fid:\n",
        "                    fid.write(r.content)\n",
        "                print(\"Download complete.\")\n",
        "\n",
        "    # Step 2: Load the dataset\n",
        "    dat = np.load(fname, allow_pickle=True).item()\n",
        "    print(\"Dataset keys:\", dat.keys())\n",
        "    data_matrix = dat['sresp']  # Neural response matrix\n",
        "    print(\"Neural response matrix shape:\", data_matrix.shape)\n",
        "    print(\"Number of features:\", len(dat['stat']))\n",
        "\n",
        "    # Step 3: Visualize behavioral data\n",
        "    plt.figure(figsize=(15, 3))\n",
        "\n",
        "    # Pupil area\n",
        "    ax = plt.subplot(1, 5, 1)\n",
        "    plt.plot(dat['pupilArea'][:500, 0])\n",
        "    ax.set(xlabel='Timepoints', ylabel='Pupil Area')\n",
        "\n",
        "    # Pupil center of mass (COM)\n",
        "    ax = plt.subplot(1, 5, 2)\n",
        "    plt.plot(dat['pupilCOM'][:500, :])\n",
        "    ax.set(xlabel='Timepoints', ylabel='Pupil XY Position')\n",
        "\n",
        "    # Face SVD component #0\n",
        "    ax = plt.subplot(1, 5, 3)\n",
        "    plt.plot(dat['beh_svd_time'][:500, 0])\n",
        "    ax.set(xlabel='Timepoints', ylabel='Face SVD #0')\n",
        "\n",
        "    # Face SVD component #1\n",
        "    ax = plt.subplot(1, 5, 4)\n",
        "    plt.plot(dat['beh_svd_time'][:500, 1])\n",
        "    ax.set(xlabel='Timepoints', ylabel='Face SVD #1')\n",
        "\n",
        "    # Scatter plot of Face SVD components\n",
        "    ax = plt.subplot(1, 5, 5)\n",
        "    plt.scatter(dat['beh_svd_time'][:, 0], dat['beh_svd_time'][:, 1], s=1)\n",
        "    ax.set(xlabel='Face SVD #0', ylabel='Face SVD #1')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Return the neural response matrix\n",
        "    return data_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "BT2y3ZEaAXEg"
      },
      "source": [
        "\n",
        "dat has fields:\n",
        "* `dat['sresp']`: neurons by timebins, a.k.a. the neural response data (11983 by 7018). Timebin = 1.2 sec.\n",
        "* `dat['run']`: timebins by 1, a.k.a. the running speed of the animal in a.u.\n",
        "* `dat['xyz']`: three-dimensional position of each neuron in the brain.\n",
        "* `dat['pupilArea']`: timebins by 1, see FaceMap for details on the pupil tracker\n",
        "* `dat['pupilCOM']`: timebins by 2, correspond to X and Y position. See FaceMap for details on the pupil tracker.\n",
        "* `dat['beh_svd_time']`: timebins by 1,000. Timecourses of behavioral SVDs extracted from face movies. See FaceMap for details.\n",
        "* `dat['beh_svd_mask']`: 240 by 320 by 1,000. Spatial masks of behavioral SVDs. Roughly normalized to each other.\n",
        "* `dat['beh_svd_mask'] @ dat['beh_svd_time'][T, :]`:  reconstruction of the face movie (its motion energy) for frame T\n",
        "* `dat['stat']`: 1 by neurons, some statistics for each neuron, see Suite2p for full documentation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_data_matrix(data_matrix):\n",
        "    \"\"\"\n",
        "    Perform a series of analyses on the given data matrix, including visualization,\n",
        "    PCA, covariance analysis, and residual analysis.\n",
        "\n",
        "    Parameters:\n",
        "    - data_matrix (numpy.ndarray): Input data matrix.\n",
        "\n",
        "    Returns:\n",
        "    - results (dict): Dictionary containing PCA components, explained variance,\n",
        "      covariance matrices, and other intermediate results.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Step 1: Heatmap of the raw data\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(np.log(data_matrix + 1), aspect='auto', cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title(\"Heatmap of Data Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Step 2: PCA on the raw data\n",
        "    pca = PCA()\n",
        "    transformed_data = pca.fit_transform(data_matrix)  # Transformed data\n",
        "    explained_variance = pca.explained_variance_ratio_  # Explained variance ratios\n",
        "    results['explained_variance'] = explained_variance\n",
        "\n",
        "    # Log-log plot of explained variance\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.loglog(\n",
        "        np.arange(1, len(explained_variance) + 1),\n",
        "        explained_variance,\n",
        "        marker='o',\n",
        "        linestyle='-',\n",
        "        label='Explained Variance'\n",
        "    )\n",
        "    plt.xlabel('Principal Component Index (log scale)', fontsize=12)\n",
        "    plt.ylabel('Explained Variance (log scale)', fontsize=12)\n",
        "    plt.title('Log-Log Plot of Explained Variance', fontsize=14)\n",
        "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Step 3: Covariance matrix of the original data\n",
        "    # normalized_matrix = (data_matrix - data_matrix.mean(axis=1, keepdims=True)) / data_matrix.std(axis=1, keepdims=True)\n",
        "    normalized_cov_matrix = compute_covariance(data_matrix)\n",
        "\n",
        "    results['normalized_cov_matrix'] = normalized_cov_matrix\n",
        "\n",
        "    # Visualize the covariance matrix\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(normalized_cov_matrix, aspect='auto', cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title(\"Covariance Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Histogram of covariance matrix elements\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.hist(\n",
        "        normalized_cov_matrix.flatten(),\n",
        "        bins=np.linspace(-1, 1, 101),\n",
        "        alpha=0.7,\n",
        "        color='blue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    plt.xlabel('Covariance Matrix Elements', fontsize=12)\n",
        "    plt.ylabel('Frequency', fontsize=12)\n",
        "    plt.title('Histogram of Covariance Matrix Elements (Range -1 to 1)', fontsize=14)\n",
        "    plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "    plt.show()\n",
        "\n",
        "    # Step 4: Residual analysis with top 10 PCs\n",
        "    top_10_pcs = pca.components_[:10]\n",
        "    residual_data = data_matrix - data_matrix @ top_10_pcs.T @ top_10_pcs  # Remove top 10 PCs\n",
        "    #normalized_matrix = (residual_data - residual_data.mean(axis=1, keepdims=True)) / residual_data.std(axis=1, keepdims=True)\n",
        "\n",
        "    residual_cov_matrix = compute_covariance(residual_data)\n",
        "    results['residual_cov_matrix'] = residual_cov_matrix\n",
        "\n",
        "    # Histogram of residual covariance matrix elements\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.hist(\n",
        "        residual_cov_matrix.flatten(),\n",
        "        bins=np.linspace(-0.5, 0.5, 101),\n",
        "        alpha=0.7,\n",
        "        color='orange',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    plt.xlabel('Residual Covariance Matrix Elements', fontsize=12)\n",
        "    plt.ylabel('Frequency', fontsize=12)\n",
        "    plt.title('Histogram of Residual Covariance Matrix Elements (Range -0.5 to 0.5)', fontsize=14)\n",
        "    plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
        "    plt.show()\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "N_e5c1PBrlaY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Below this starts the code that just analyses the relevant matrices"
      ],
      "metadata": {
        "id": "PtnFE25qlt86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary to map dataset names to their respective loading functions\n",
        "dataset_loaders = {\n",
        "    \"RNASeq\": load_rnaseq_data,\n",
        "    \"HEXACO\": load_hexaco_data,\n",
        "    \"PDA360\": load_pda360_data,\n",
        "    \"Stringer\": load_stringer_data\n",
        "}\n",
        "\n",
        "# Initialize a dictionary to store results for each dataset\n",
        "all_results = {}\n",
        "\n",
        "# Loop through each dataset\n",
        "for dataset_name, loader_function in dataset_loaders.items():\n",
        "    print(f\"Loading and analyzing dataset: {dataset_name}\")\n",
        "\n",
        "    # Load the data matrix\n",
        "    data_matrix = loader_function()\n",
        "    print(f\"{dataset_name} data matrix shape: {data_matrix.shape}\")\n",
        "\n",
        "    # Analyze the data matrix\n",
        "    analysis_results = analyze_data_matrix(data_matrix)\n",
        "\n",
        "    # Store the results\n",
        "    all_results[dataset_name] = analysis_results\n",
        "    print(f\"Analysis complete for {dataset_name}.\\n\")\n",
        "\n",
        "# all_results now contains the analysis outputs for all datasets\n"
      ],
      "metadata": {
        "id": "Yl3qfVwPpxZy",
        "outputId": "12f802f0-9ca5-49a8-d362-e05e992e269b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and analyzing dataset: RNASeq\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "load_stringer_spontaneous",
      "provenance": [],
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}